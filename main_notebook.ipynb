{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe96416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8efdee06c7e4c54a32ac09b2b303e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>ü§ñ Data Analyst Agent</h2>'), HTML(value='<p>Upload a file (CSV, Excel, TXT, PDF‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# main_notebook.ipynb\n",
    "\n",
    "# Cell 1: Imports and Setup\n",
    "import os\n",
    "import shutil\n",
    "from IPython.display import display, Markdown, Image as IPImage, clear_output\n",
    "import ipywidgets as widgets\n",
    "from PIL import Image as PILImage # Renamed to avoid conflict\n",
    "import io\n",
    "import base64\n",
    "import pandas as pd # For displaying DataFrames nicely\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our utility functions\n",
    "from utils.file_parser import parse_file\n",
    "from utils.llm_handler import get_llm_response, MODEL_NAME\n",
    "from utils.code_executor import execute_python_code\n",
    "\n",
    "# Create necessary directories if they don't exist\n",
    "os.makedirs(\"uploads\", exist_ok=True)\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# --- Global State (simulating a session) ---\n",
    "# In a real app, this would be session-specific\n",
    "current_file_path = None\n",
    "current_file_content = None\n",
    "current_file_type = None # 'text', 'dataframe', 'error'\n",
    "conversation_history = [] # List of {\"role\": \"user/assistant\", \"content\": \"message\"}\n",
    "MAX_HISTORY_LEN = 10 # Keep last 5 Q/A pairs (10 messages)\n",
    "\n",
    "# --- System Prompt for the Agent ---\n",
    "SYSTEM_PROMPT = f\"\"\"You are a helpful Data Analyst Agent. Your name is \"Llama Analyst\". \n",
    "You are using the LLM model: {MODEL_NAME}.\n",
    "You can analyze data from uploaded files, answer questions about them, and generate Python code for analysis and visualization.\n",
    "\n",
    "File Context:\n",
    "- If a file is uploaded, I will provide you with its content or a summary.\n",
    "- For tabular data (CSV/Excel), I will give you the column names, data types, and the head of the DataFrame. The DataFrame will be available as `df` in the Python execution environment.\n",
    "- For text-based files (TXT, DOCX, PDF, OCR'd Images), I will provide the extracted text.\n",
    "\n",
    "Your Task:\n",
    "1.  Understand the user's question in the context of the provided file data.\n",
    "2.  If the question is a general greeting or unrelated to data analysis, answer politely.\n",
    "3.  For questions about textual data: Answer based on the text provided.\n",
    "4.  For data analysis or visualization requests on tabular data:\n",
    "    a.  Generate **ONLY Python code** to perform the task.\n",
    "    b.  The code should use a DataFrame named `df`.\n",
    "    c.  Available libraries: pandas (as pd), numpy (as np), matplotlib.pyplot (as plt), seaborn (as sns).\n",
    "    d.  **IMPORTANT FOR PLOTS**: \n",
    "        - Generate plots using matplotlib/seaborn. \n",
    "        - **DO NOT call `plt.show()`**. The system will capture the plot automatically.\n",
    "        - Ensure plots have titles and axis labels for clarity.\n",
    "        - Example plot: `plt.figure(figsize=(10,6)); sns.histplot(df['column_name']); plt.title('Distribution');`\n",
    "    e.  For analysis that results in data (e.g., mean, count), print the result using `print()`. Example: `print(df['age'].mean())`\n",
    "    f.  Enclose the Python code **strictly** within triple backticks, like this:\n",
    "        ```python\n",
    "        # Your python code here\n",
    "        print(df.describe())\n",
    "        ```\n",
    "5.  If the user's request is ambiguous or you need more information, ask clarifying questions.\n",
    "6.  If you cannot answer or perform the task, state so clearly and explain why.\n",
    "7.  Keep your responses concise and to the point.\n",
    "8.  After I execute your Python code, I will show you the `stdout`, `stderr` (if any), and the generated plot (if any). You can then interpret these results for the user or suggest next steps.\n",
    "\"\"\"\n",
    "\n",
    "# Add system prompt to history\n",
    "conversation_history.append({\"role\": \"system\", \"content\": SYSTEM_PROMPT})\n",
    "\n",
    "# Cell 2: UI Elements (Widgets)\n",
    "file_upload_widget = widgets.FileUpload(\n",
    "    accept='.doc, .txt, .xlsx, .csv, .pdf, .png, .jpg, .jpeg',  # Accepted file types\n",
    "    multiple=False,  # Allow only one file at a time\n",
    "    description='Upload File'\n",
    ")\n",
    "\n",
    "text_input_widget = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Ask a question about the uploaded file or general queries...',\n",
    "    description='Your Query:',\n",
    "    layout={'width': '80%', 'height': '100px'}\n",
    ")\n",
    "\n",
    "submit_button = widgets.Button(\n",
    "    description='Submit Query',\n",
    "    button_style='success',\n",
    "    tooltip='Click to submit your query to the agent'\n",
    ")\n",
    "\n",
    "clear_button = widgets.Button(\n",
    "    description='Clear Conversation',\n",
    "    button_style='warning',\n",
    "    tooltip='Clear the conversation history'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output() # To display results\n",
    "\n",
    "# Cell 3: Helper Functions for UI and Agent Logic\n",
    "\n",
    "def display_message(role, message_content, is_code=False, is_plot_path=None, is_df=False):\n",
    "    \"\"\"Display messages with appropriate formatting\"\"\"\n",
    "    with output_area:\n",
    "        if role == \"user\":\n",
    "            display(Markdown(f\"üë§ **You:** {message_content}\"))\n",
    "        elif role == \"assistant\":\n",
    "            if is_plot_path:\n",
    "                display(Markdown(f\"ü§ñ **Llama Analyst:** Here is the visualization:\"))\n",
    "                try:\n",
    "                    display(IPImage(filename=is_plot_path))\n",
    "                except Exception as e:\n",
    "                    display(Markdown(f\"‚ö†Ô∏è Error displaying plot: {str(e)}\"))\n",
    "            elif is_df and isinstance(message_content, pd.DataFrame):\n",
    "                display(Markdown(f\"ü§ñ **Llama Analyst:** Here's a part of the data:\"))\n",
    "                display(message_content.head()) # Display DataFrame\n",
    "            elif is_code:\n",
    "                display(Markdown(f\"ü§ñ **Llama Analyst:** (Generated Code)\\n```python\\n{message_content}\\n```\"))\n",
    "            else:\n",
    "                display(Markdown(f\"ü§ñ **Llama Analyst:** {message_content}\"))\n",
    "        elif role == \"system_code_output\":\n",
    "            display(Markdown(f\"‚öôÔ∏è **Code Output:**\\n```\\n{message_content}\\n```\"))\n",
    "        elif role == \"system_error\":\n",
    "             display(Markdown(f\"‚ö†Ô∏è **System Error:** {message_content}\"))\n",
    "\n",
    "\n",
    "def add_to_history(role, content):\n",
    "    \"\"\"Add message to conversation history with size management\"\"\"\n",
    "    global conversation_history\n",
    "    conversation_history.append({\"role\": role, \"content\": content})\n",
    "    # Keep history to a manageable size\n",
    "    if len(conversation_history) > MAX_HISTORY_LEN + 1: # +1 for system prompt\n",
    "        # Keep system prompt, remove older messages from the middle\n",
    "        conversation_history = [conversation_history[0]] + conversation_history[-(MAX_HISTORY_LEN):]\n",
    "\n",
    "def extract_python_code(llm_response):\n",
    "    \"\"\"Extract Python code from LLM response\"\"\"\n",
    "    if \"```python\" in llm_response:\n",
    "        try:\n",
    "            code = llm_response.split(\"```python\")[1].split(\"```\")[0].strip()\n",
    "            return code\n",
    "        except IndexError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def get_df_summary(df):\n",
    "    \"\"\"Creates a string summary of the DataFrame for the LLM.\"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        return \"No DataFrame available or error in loading.\"\n",
    "    \n",
    "    try:\n",
    "        # Use an in-memory text buffer to capture DataFrame.info()\n",
    "        buffer = io.StringIO()\n",
    "        df.info(buf=buffer)\n",
    "        df_info_str = buffer.getvalue()\n",
    "        \n",
    "        summary = f\"The DataFrame has {df.shape[0]} rows and {df.shape[1]} columns.\\n\"\n",
    "        summary += \"Column names, data types, and non-null counts:\\n\"\n",
    "        summary += df_info_str + \"\\n\\n\"\n",
    "        summary += \"First 5 rows (df.head()):\\n\"\n",
    "        summary += df.head().to_string() + \"\\n\"\n",
    "        \n",
    "        # Only include describe() if DataFrame is not too large\n",
    "        if df.shape[0] > 0:\n",
    "            summary += \"Basic descriptive statistics (df.describe()):\\n\"\n",
    "            summary += df.describe(include='all').to_string() # include='all' for non-numeric too\n",
    "        \n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        return f\"Error generating DataFrame summary: {str(e)}\"\n",
    "\n",
    "# Cell 4: Event Handlers\n",
    "\n",
    "def on_file_upload_change(change):\n",
    "    \"\"\"Handle file upload events\"\"\"\n",
    "    global current_file_path, current_file_content, current_file_type, conversation_history\n",
    "    \n",
    "    # Clear previous file state and relevant history\n",
    "    current_file_path = None\n",
    "    current_file_content = None\n",
    "    current_file_type = None\n",
    "    # Reset history, but keep system prompt\n",
    "    conversation_history = [conversation_history[0]] if conversation_history and conversation_history[0]['role'] == 'system' else [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "\n",
    "    uploaded_files = file_upload_widget.value\n",
    "    if not uploaded_files:\n",
    "        with output_area:\n",
    "            clear_output(wait=True) # Clear previous outputs\n",
    "            display(Markdown(\"No file selected or upload cleared.\"))\n",
    "        return\n",
    "\n",
    "    # Handle the tuple structure of uploaded files\n",
    "    # uploaded_files is a tuple of FileUpload objects\n",
    "    if isinstance(uploaded_files, tuple) and len(uploaded_files) > 0:\n",
    "        uploaded_file = uploaded_files[0]  # Get the first (and only) file\n",
    "        filename = uploaded_file['name']\n",
    "        file_content = uploaded_file['content']  # This is in bytes\n",
    "    else:\n",
    "        with output_area:\n",
    "            clear_output(wait=True)\n",
    "            display_message(\"system_error\", \"Invalid file upload format.\")\n",
    "        return\n",
    "\n",
    "    # Save the uploaded file temporarily to pass its path to parsers\n",
    "    temp_file_path = os.path.join(\"uploads\", filename)\n",
    "    try:\n",
    "        with open(temp_file_path, 'wb') as f:\n",
    "            f.write(file_content)\n",
    "        \n",
    "        current_file_path = temp_file_path # Store path of the saved temp file\n",
    "\n",
    "        with output_area:\n",
    "            clear_output(wait=True) # Clear previous outputs\n",
    "            display(Markdown(f\"üìÑ **File Uploaded:** {filename}\"))\n",
    "            \n",
    "            content, content_type = parse_file(current_file_path)\n",
    "            \n",
    "            if content_type == \"error\":\n",
    "                display_message(\"system_error\", content)\n",
    "                current_file_content = None\n",
    "                current_file_type = None\n",
    "            else:\n",
    "                current_file_content = content\n",
    "                current_file_type = content_type\n",
    "                \n",
    "                if current_file_type == \"dataframe\":\n",
    "                    if isinstance(current_file_content, pd.DataFrame):\n",
    "                        display(Markdown(f\"üìä **Data Preview (first 5 rows):**\"))\n",
    "                        display(current_file_content.head())\n",
    "                        add_to_history(\"system\", f\"File '{filename}' uploaded. It's a DataFrame. Summary:\\n{get_df_summary(current_file_content)}\")\n",
    "                    else: # Should be an error message from parser\n",
    "                        display_message(\"system_error\", f\"Expected DataFrame but got: {str(current_file_content)}\")\n",
    "                        current_file_content = None # Invalidate\n",
    "                        current_file_type = \"error\"\n",
    "                elif current_file_type == \"text\":\n",
    "                    preview_text = current_file_content[:500] if len(current_file_content) > 500 else current_file_content\n",
    "                    display(Markdown(f\"üìú **Text Preview:**\\n```\\n{preview_text}{'...' if len(current_file_content) > 500 else ''}\\n```\"))\n",
    "                    add_to_history(\"system\", f\"File '{filename}' uploaded. It's text content. Length: {len(current_file_content)} chars.\")\n",
    "                else:\n",
    "                    display_message(\"system_error\", f\"Unknown content type: {current_file_type}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        with output_area:\n",
    "            display_message(\"system_error\", f\"Error processing file: {str(e)}\")\n",
    "        print(f\"Debug - Exception: {e}\")\n",
    "        print(f\"Debug - uploaded_files type: {type(uploaded_files)}\")\n",
    "        print(f\"Debug - uploaded_files content: {uploaded_files}\")\n",
    "\n",
    "\n",
    "file_upload_widget.observe(on_file_upload_change, names='value')\n",
    "\n",
    "def on_submit_button_clicked(b):\n",
    "    \"\"\"Handle submit button clicks\"\"\"\n",
    "    global conversation_history, current_file_content, current_file_type\n",
    "\n",
    "    user_query = text_input_widget.value.strip()\n",
    "    if not user_query:\n",
    "        with output_area:\n",
    "            display_message(\"system_error\", \"Please enter a query.\")\n",
    "        return\n",
    "\n",
    "    with output_area: # Display user query immediately\n",
    "        display_message(\"user\", user_query)\n",
    "    \n",
    "    add_to_history(\"user\", user_query)\n",
    "    text_input_widget.value = \"\" # Clear input field\n",
    "\n",
    "    # Get LLM response\n",
    "    try:\n",
    "        llm_response_text = get_llm_response(user_query, conversation_history=conversation_history, max_tokens=1500)\n",
    "        \n",
    "        if llm_response_text.startswith(\"Error:\"):\n",
    "            with output_area:\n",
    "                display_message(\"system_error\", llm_response_text)\n",
    "            add_to_history(\"assistant\", f\"Error occurred: {llm_response_text}\")\n",
    "            return\n",
    "\n",
    "        add_to_history(\"assistant\", llm_response_text) # Add raw LLM response to history\n",
    "\n",
    "        # Check if LLM generated Python code\n",
    "        python_code = extract_python_code(llm_response_text)\n",
    "\n",
    "        if python_code:\n",
    "            with output_area:\n",
    "                display(Markdown(\"ü§ñ **Llama Analyst:** I've generated some Python code to address your query. Here it is:\"))\n",
    "                display(Markdown(f\"```python\\n{python_code}\\n```\"))\n",
    "                display(Markdown(\"‚öôÔ∏è **Executing code...**\"))\n",
    "\n",
    "            if current_file_type == \"dataframe\" and isinstance(current_file_content, pd.DataFrame):\n",
    "                stdout_val, stderr_val, plot_b64 = execute_python_code(python_code, df=current_file_content)\n",
    "                \n",
    "                # Display outputs\n",
    "                if stdout_val:\n",
    "                    with output_area:\n",
    "                        display(Markdown(f\"**Code Output:**\\n```\\n{stdout_val}\\n```\"))\n",
    "                \n",
    "                if stderr_val:\n",
    "                    with output_area:\n",
    "                        display(Markdown(f\"‚ö†Ô∏è **Code Errors:**\\n```\\n{stderr_val}\\n```\"))\n",
    "                \n",
    "                # Handle plot display\n",
    "                if plot_b64:\n",
    "                    plot_filename = f\"plot_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "                    plot_save_path = os.path.join(\"outputs\", plot_filename)\n",
    "                    try:\n",
    "                        with open(plot_save_path, \"wb\") as f:\n",
    "                            f.write(base64.b64decode(plot_b64))\n",
    "                        with output_area:\n",
    "                            display(Markdown(f\"üñºÔ∏è **Generated Plot:** (Saved as {plot_filename})\"))\n",
    "                            display(IPImage(filename=plot_save_path))\n",
    "                    except Exception as e:\n",
    "                        with output_area:\n",
    "                            display_message(\"system_error\", f\"Error saving/displaying plot: {str(e)}\")\n",
    "\n",
    "                # Follow-up interpretation if needed\n",
    "                if stdout_val or stderr_val or plot_b64:\n",
    "                    interpretation_prompt = f\"\"\"The Python code you provided was executed.\n",
    "Code:\n",
    "```python\n",
    "{python_code}\n",
    "```\n",
    "\n",
    "Results:\n",
    "- Output: {stdout_val if stdout_val else 'None'}\n",
    "- Errors: {stderr_val if stderr_val else 'None'}\n",
    "- Plot generated: {'Yes' if plot_b64 else 'No'}\n",
    "\n",
    "Please provide a brief interpretation of these results for the user.\"\"\"\n",
    "\n",
    "                    try:\n",
    "                        interpretation = get_llm_response(interpretation_prompt, conversation_history=conversation_history[-5:], max_tokens=500)\n",
    "                        if not interpretation.startswith(\"Error:\"):\n",
    "                            with output_area:\n",
    "                                display(Markdown(f\"ü§ñ **Llama Analyst:** {interpretation}\"))\n",
    "                    except Exception as e:\n",
    "                        with output_area:\n",
    "                            display_message(\"system_error\", f\"Error getting interpretation: {str(e)}\")\n",
    "            else:\n",
    "                with output_area:\n",
    "                    display_message(\"system_error\", \"Code execution requires a DataFrame. Please upload a CSV or Excel file first.\")\n",
    "        else:\n",
    "            # No code generated, just display the LLM response\n",
    "            with output_area:\n",
    "                display_message(\"assistant\", llm_response_text)\n",
    "                \n",
    "    except Exception as e:\n",
    "        with output_area:\n",
    "            display_message(\"system_error\", f\"Error processing query: {str(e)}\")\n",
    "\n",
    "def on_clear_button_clicked(b):\n",
    "    \"\"\"Clear conversation history and output\"\"\"\n",
    "    global conversation_history, current_file_content, current_file_type, current_file_path\n",
    "    \n",
    "    # Reset global state\n",
    "    conversation_history = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "    current_file_content = None\n",
    "    current_file_type = None\n",
    "    current_file_path = None\n",
    "    \n",
    "    # Clear output area\n",
    "    with output_area:\n",
    "        clear_output(wait=True)\n",
    "        display(Markdown(\"üîÑ **Conversation cleared. Ready for new queries!**\"))\n",
    "\n",
    "submit_button.on_click(on_submit_button_clicked)\n",
    "clear_button.on_click(on_clear_button_clicked)\n",
    "\n",
    "# Cell 5: Display the UI\n",
    "def create_ui():\n",
    "    \"\"\"Create and display the complete UI\"\"\"\n",
    "    ui_box = widgets.VBox([\n",
    "        widgets.HTML(\"<h2>ü§ñ Data Analyst Agent</h2>\"),\n",
    "        widgets.HTML(\"<p>Upload a file (CSV, Excel, TXT, PDF, etc.) and ask questions about it!</p>\"),\n",
    "        file_upload_widget,\n",
    "        text_input_widget,\n",
    "        widgets.HBox([submit_button, clear_button]),\n",
    "        output_area\n",
    "    ])\n",
    "    \n",
    "    display(ui_box)\n",
    "    \n",
    "    # Initial message\n",
    "    with output_area:\n",
    "        display(Markdown(\"üëã **Welcome to Data Analyst Agent!** Upload a file and start asking questions.\"))\n",
    "\n",
    "# Cell 6: Initialize the application\n",
    "if __name__ == \"__main__\":\n",
    "    create_ui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1bf46e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
